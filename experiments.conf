# Word embeddings.
glove_300d {
  path = glove.840B.300d.txt
  size = 300
  format = txt
  lowercase = false
}
glove_300d_filtered {
  path = all.wikicoref.glove.840B.300d.txt.filtered
  size = 300
  format = txt
  lowercase = false
}
glove_300d_filtered_no_pt {
  path = glove.840B.300d.no_pt.txt.filtered
  size = 300
  format = txt
  lowercase = false
}
glove_300d_filtered_no_wb {
  path = glove.840B.300d.no_wb.txt.filtered
  size = 300
  format = txt
  lowercase = false
}
turian_50d {
  path = turian.50d.txt
  size = 50
  format = txt
  lowercase = false
}

# Compute clusters.
nlp {
  addresses {
    ps = [nlp2:2222]
    worker = [n01:2222, n02:2222, n03:2222, n04:2222, n05:2222, n07:2222, n08:2222, n09:2222, n10:2222, n11:2222, n12:2222, n13:2222, n14:2222, n15:2222, n16:2222]
  }
  gpus = [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]
}
appositive {
  addresses {
    ps = [localhost:2222]
    worker = [localhost:2223, localhost:2224]
  }
  gpus = [0, 1]
}

# Main configuration.
best {
  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  embeddings = [${glove_300d_filtered}, ${turian_50d}]
  lstm_size = 200
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_mention_width = 10
  use_metadata = true
  use_features = true
  model_heads = true

  # New features
  use_pos_tag = true
  use_ner_tag = true
  pos_tag_path = add_ner/pos_tag_list.txt

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = add_ner/all.train.english.jsonlines
  eval_path = add_ner/all.dev.english.jsonlines

  conll_eval_path = add_ner/dev.english.v4_auto_conll
  genres = [bc, bn, mz, nw, pt, tc, wb]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
  cluster = ${appositive}
}

# Multiple full models for ensembling.
best0 = ${best}
best1 = ${best}
best2 = ${best}
best3 = ${best}
best4 = ${best}

# Ablations.
glove = ${best} {
  embeddings = [${glove_300d_filtered}]
}
turian = ${best} {
  embeddings = [${turian_50d}]
}
nochar = ${best} {
  char_embedding_size = -1
}
nometa = ${best} {
  use_metadata = false
}
noheads = ${best} {
  model_heads = false
}
nofeatures = ${best} {
  use_features = false
}

mini = ${best}{
  train_path = add_ner/minitrain.jsonlines
  eval_path = add_ner/minidev.jsonlines
  conll_eval_path = add_ner/minidev.v4_auto_conll
}

mini_no_pos_tag = ${mini}{
  use_pos_tag = false
}

# For evaluation. Do not use for training (i.e. only for decoder.py, ensembler.py, visualize.py and demo.py). Rename `best0` directory to `final`.
final = ${best} {
  embeddings = [${glove_300d}, ${turian_50d}]
  #eval_path = test.english.jsonlines
  #conll_eval_path = test.english.v4_gold_conll
  eval_path = wikicoref.test.english.jsonlines
  conll_eval_path = wikicoref.test.english.v4_gold_conll
  #eval_path = test_files/wb.test.english.jsonlines
  #conll_eval_path = test_files/wb.test.english.v4_gold_conll
  genres = [bc, bn, mz, nw, pt, tc, wb]
}

wikicoref = ${best}{
  genres = [bc, bn, mz, nw, pt, tc, wb, wc]
}

final_wikicoref = ${wikicoref}{
  embeddings = [${glove_300d}, ${turian_50d}]
  eval_path = wikicoref.test.english.jsonlines
  conll_eval_path = wikicoref.test.english.v4_gold_conll
}

final_nometa = ${nometa}{
  embeddings = [${glove_300d}, ${turian_50d}]
  eval_path = wikicoref.test.english.jsonlines
  conll_eval_path = wikicoref.test.english.v4_gold_conll
  genres = [bc, bn, mz, nw, pt, tc, wb]
}

# Main configuration.
no_pt {
  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  embeddings = [${glove_300d_filtered_no_pt}, ${turian_50d}]
  lstm_size = 200
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_mention_width = 10
  use_metadata = true
  use_features = true
  model_heads = true

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = no_pt.train.english.jsonlines
  eval_path = no_pt.dev.english.jsonlines
  conll_eval_path = no_pt.dev.english.v4_auto_conll
  genres = [bc, bn, mz, nw, pt, tc, wb]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
  cluster = ${appositive}
}

# Main configuration.
no_wb {
  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  mention_ratio = 0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "char_vocab.english.txt"
  embeddings = [${glove_300d_filtered_no_wb}, ${turian_50d}]
  lstm_size = 200
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_mention_width = 10
  use_metadata = true
  use_features = true
  model_heads = true

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  # Other.
  train_path = no_wb.train.english.jsonlines
  eval_path = no_wb.dev.english.jsonlines
  conll_eval_path = no_wb.dev.english.v4_auto_conll
  genres = [bc, bn, mz, nw, pt, tc, wb]
  eval_frequency = 1000
  report_frequency = 100
  log_root = logs
  cluster = ${appositive}
}

final_no_pt = ${final} {
  eval_path = test_files/pt.test.english.jsonlines
  conll_eval_path = test_files/pt.test.english.v4_gold_conll
}

final_no_wb = ${final} {
  eval_path = test_files/wb.test.english.jsonlines
  conll_eval_path = test_files/wb.test.english.v4_gold_conll
}

final_no_wb2 = ${no_wb} {
  eval_path = test_files/wb.test.english.jsonlines
  conll_eval_path = test_files/wb.test.english.v4_gold_conll
}